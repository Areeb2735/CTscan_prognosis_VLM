{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:261: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/vector_quantize_pytorch/vector_quantize_pytorch.py:391: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  @autocast(enabled = False)\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/env/lib/python3.8/site-packages/transformers/modeling_utils.py:463: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT_CLIP/ct_clip/ct_clip.py:598: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  pt = torch.load(str(path))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lora_model(\n",
       "  (clip): PeftModel(\n",
       "    (base_model): LoraModel(\n",
       "      (model): CTCLIP(\n",
       "        (text_transformer): BertModel(\n",
       "          (embeddings): BertEmbeddings(\n",
       "            (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "            (position_embeddings): Embedding(512, 768)\n",
       "            (token_type_embeddings): Embedding(2, 768)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.25, inplace=False)\n",
       "          )\n",
       "          (encoder): BertEncoder(\n",
       "            (layer): ModuleList(\n",
       "              (0-11): 12 x BertLayer(\n",
       "                (attention): BertAttention(\n",
       "                  (self): BertSelfAttention(\n",
       "                    (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.25, inplace=False)\n",
       "                  )\n",
       "                  (output): BertSelfOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                    (dropout): Dropout(p=0.25, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "                (intermediate): BertIntermediate(\n",
       "                  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                  (intermediate_act_fn): GELUActivation()\n",
       "                )\n",
       "                (output): BertOutput(\n",
       "                  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                  (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                  (dropout): Dropout(p=0.25, inplace=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (pooler): BertPooler(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (activation): Tanh()\n",
       "          )\n",
       "        )\n",
       "        (visual_transformer): CTViT(\n",
       "          (spatial_rel_pos_bias): ContinuousPositionBias(\n",
       "            (net): ModuleList(\n",
       "              (0): Sequential(\n",
       "                (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                (1): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "              (1): Sequential(\n",
       "                (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "                (1): LeakyReLU(negative_slope=0.1)\n",
       "              )\n",
       "              (2): Linear(in_features=512, out_features=8, bias=True)\n",
       "            )\n",
       "          )\n",
       "          (to_patch_emb_first_frame): Sequential(\n",
       "            (0): Rearrange('b c 1 (h p1) (w p2) -> b 1 h w (c p1 p2)', p1=20, p2=20)\n",
       "            (1): LayerNorm((400,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): Linear(in_features=400, out_features=512, bias=True)\n",
       "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (to_patch_emb): Sequential(\n",
       "            (0): Rearrange('b c (t pt) (h p1) (w p2) -> b t h w (c pt p1 p2)', p1=20, p2=20, pt=10)\n",
       "            (1): LayerNorm((4000,), eps=1e-05, elementwise_affine=True)\n",
       "            (2): Linear(in_features=4000, out_features=512, bias=True)\n",
       "            (3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (enc_spatial_transformer): Transformer(\n",
       "            (layers): ModuleList(\n",
       "              (0-3): 4 x ModuleList(\n",
       "                (0): PEG(\n",
       "                  (dsconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=512)\n",
       "                )\n",
       "                (1): Attention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): LayerNorm()\n",
       "                  (context_norm): LayerNorm()\n",
       "                  (to_q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=256, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.2, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_kv): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.2, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_out): Linear(in_features=256, out_features=512, bias=False)\n",
       "                )\n",
       "                (2): None\n",
       "                (3): Sequential(\n",
       "                  (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (1): Linear(in_features=512, out_features=2730, bias=False)\n",
       "                  (2): GEGLU()\n",
       "                  (3): Dropout(p=0.0, inplace=False)\n",
       "                  (4): Linear(in_features=1365, out_features=512, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm_out): LayerNorm()\n",
       "          )\n",
       "          (enc_temporal_transformer): Transformer(\n",
       "            (layers): ModuleList(\n",
       "              (0-3): 4 x ModuleList(\n",
       "                (0): PEG(\n",
       "                  (dsconv): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), groups=512)\n",
       "                )\n",
       "                (1): Attention(\n",
       "                  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "                  (norm): LayerNorm()\n",
       "                  (context_norm): LayerNorm()\n",
       "                  (to_q): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=256, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.2, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_kv): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=512, out_features=512, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.2, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (to_out): Linear(in_features=256, out_features=512, bias=False)\n",
       "                )\n",
       "                (2): None\n",
       "                (3): Sequential(\n",
       "                  (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                  (1): Linear(in_features=512, out_features=2730, bias=False)\n",
       "                  (2): GEGLU()\n",
       "                  (3): Dropout(p=0.0, inplace=False)\n",
       "                  (4): Linear(in_features=1365, out_features=512, bias=False)\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (norm_out): LayerNorm()\n",
       "          )\n",
       "          (vq): VectorQuantize(\n",
       "            (project_in): Identity()\n",
       "            (project_out): Identity()\n",
       "            (_codebook): CosineSimCodebook()\n",
       "          )\n",
       "          (to_pixels_first_frame): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=400, bias=True)\n",
       "            (1): Rearrange('b 1 h w (c p1 p2) -> b c 1 (h p1) (w p2)', p1=20, p2=20)\n",
       "          )\n",
       "          (to_pixels): Sequential(\n",
       "            (0): Linear(in_features=512, out_features=4000, bias=True)\n",
       "            (1): Rearrange('b t h w (c pt p1 p2) -> b c (t pt) (h p1) (w p2)', p1=20, p2=20, pt=10)\n",
       "          )\n",
       "        )\n",
       "        (to_text_latent): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (to_visual_latent): Linear(in_features=294912, out_features=512, bias=False)\n",
       "        (to_text_latent_extra): Linear(in_features=768, out_features=512, bias=False)\n",
       "        (to_visual_latent_extra): Linear(in_features=294912, out_features=512, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (fc_2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (fc_3): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (mtlr): MTLR(in_features=256, num_time_bins=11)\n",
       "  (batch_norm): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import Adam\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils import make_time_bins\n",
    "from utils import encode_survival, mtlr_neg_log_likelihood, make_optimizer\n",
    "from utils import mtlr_survival, mtlr_risk\n",
    "from prognosis_model import embd_model, lora_model\n",
    "\n",
    "from ct_clip import CTCLIP\n",
    "from transformer_maskgit import CTViT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from lifelines.utils import concordance_index\n",
    "from data_inference_hector import Hector_Dataset_emb, Hector_Dataset\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed) \n",
    "generator = torch.Generator().manual_seed(seed)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('microsoft/BiomedVLP-CXR-BERT-specialized',do_lower_case=True)\n",
    "text_encoder = BertModel.from_pretrained(\"microsoft/BiomedVLP-CXR-BERT-specialized\")\n",
    "\n",
    "text_encoder.resize_token_embeddings(len(tokenizer))\n",
    "text_encoder.to(device)\n",
    "\n",
    "image_encoder = CTViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 8192,\n",
    "    image_size = 480,\n",
    "    patch_size = 20,\n",
    "    temporal_patch_size = 10,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 32,\n",
    "    heads = 8\n",
    ")\n",
    "\n",
    "image_encoder.to(device)\n",
    "\n",
    "clip = CTCLIP(\n",
    "    image_encoder = image_encoder,\n",
    "    text_encoder = text_encoder,\n",
    "    dim_image = 294912,\n",
    "    dim_text = 768,\n",
    "    dim_latent = 512,\n",
    "    extra_latent_projection = False,         # whether to use separate projections for text-to-image vs image-to-text comparisons (CLOOB)\n",
    "    use_mlm=False,\n",
    "    downsample_image_embeds = False,\n",
    "    use_all_token_embeds = False,\n",
    ")\n",
    "\n",
    "clip.load(\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT-CLIP_v2.pt\")\n",
    "clip.to(device)\n",
    "\n",
    "hect_dataset = Hector_Dataset(data_folder = \"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/valid_preprocessed_hector/\",  \n",
    "                csv_file =\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/final_hector_with_text.csv\")\n",
    "\n",
    "\n",
    "train_size = int(0.8 * len(hect_dataset))  # 80% for training\n",
    "test_size = len(hect_dataset) - train_size  # 20% for testing\n",
    "train_dataset, test_dataset = random_split(hect_dataset, [train_size, test_size], generator=generator)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "df = pd.read_csv(\"/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/final_hector_with_text.csv\")\n",
    "time_bins = make_time_bins(df['RFS'].values, event = df['Relapse'].values)\n",
    "num_time_bins = len(time_bins)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    inference_mode=False, r=8, lora_alpha=64, lora_dropout=0.2, target_modules=[\"to_q\", \"to_kv\"]\n",
    ")\n",
    "\n",
    "model = lora_model(clip, device, peft_config, num_time_bins)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3188212/99424953.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/model_weights/weight_095.pth\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation of the model\n",
      "Concordance Index: 0.4298\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/model_weights/weight_095.pth\"))\n",
    "print(\"Validation of the model\")\n",
    "model.eval()\n",
    "pred_risk_all = []\n",
    "relapse_all = []\n",
    "RFS_all = []\n",
    "pred_survival_all = []\n",
    "with torch.no_grad():\n",
    "    for img_emb, text_emb, relapse, RFS, _ in test_loader:\n",
    "        img_emb = img_emb.to(device)\n",
    "        text_emb=tokenizer(text_emb, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=512).to(device)\n",
    "\n",
    "        y_pred = model(text_emb, img_emb)\n",
    "        pred_survival = mtlr_survival(y_pred).cpu().numpy()\n",
    "        pred_risk = mtlr_risk(y_pred).cpu().numpy()\n",
    "\n",
    "        pred_risk_all.append(pred_risk.item()) \n",
    "        relapse_all.append(relapse.item())\n",
    "        RFS_all.append(RFS.item())\n",
    "        pred_survival_all.append(list(pred_survival[0]))\n",
    "\n",
    "ci = concordance_index(RFS_all, -np.array(pred_risk_all), event_observed=relapse_all)\n",
    "print(f\"Concordance Index: {ci:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.9954841 , 0.98311925, ..., 0.84875095, 0.7843362 ,\n",
       "        0.4419284 ],\n",
       "       [0.99999994, 0.9804237 , 0.9392827 , ..., 0.6399225 , 0.5471909 ,\n",
       "        0.31601977],\n",
       "       [1.        , 0.9896096 , 0.96635306, ..., 0.7328142 , 0.6472257 ,\n",
       "        0.36006597],\n",
       "       ...,\n",
       "       [0.99999994, 0.9957256 , 0.9821786 , ..., 0.7130777 , 0.64634717,\n",
       "        0.29794365],\n",
       "       [1.        , 0.9987982 , 0.99350846, ..., 0.928804  , 0.8949395 ,\n",
       "        0.50034034],\n",
       "       [1.        , 0.999854  , 0.99934024, ..., 0.98197997, 0.9433391 ,\n",
       "        0.6599177 ]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred_survival_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'auc_266': 0.43548387096774194,\n",
       " 'auc_405': 0.30461922596754054,\n",
       " 'auc_782': 0.4206426484907498}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get only those RFS_all values for which relapse is 1\n",
    "\n",
    "{f\"auc_{t}\": auc[i] for i, t in enumerate(eval_times)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1362,  400,  323,  435,  196,  393, 2315,   88,  846,  526,  920,\n",
       "       4425,  410,  202,  592,  248,  330,   96])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([RFS_all[i] for i in range(len(RFS_all)) if relapse_all[i] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if any elemt is greater than 1\n",
    "\n",
    "for i in range(len(pred_survival_all)):\n",
    "    if np.array(pred_survival_all)[i].max() > 1:\n",
    "        print(i, np.array(pred_survival_all)[i].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_survival_all[15][0]\n",
    "\n",
    "# replace all the values greater than 1 with 1\n",
    "\n",
    "for i in range(len(pred_survival_all)):\n",
    "    for j in range(len(pred_survival_all[i])):\n",
    "        if pred_survival_all[i][j] > 1:\n",
    "            pred_survival_all[i][j] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(pred_survival_all)[0].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_times = np.quantile(np.array([RFS_all[i] for i in range(len(RFS_all)) if relapse_all[i] == 1]), [.25, .5, .75]).astype(int)\n",
    "\n",
    "bs = brier_score_at_times(np.array(RFS_all), np.array(pred_survival_all), np.array(relapse_all), eval_times)\n",
    "auc = roc_auc_at_times(np.array(RFS_all), np.array(pred_survival_all), np.array(relapse_all), eval_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>bs_266</th>\n",
       "      <th>bs_405</th>\n",
       "      <th>bs_782</th>\n",
       "      <th>auc_266</th>\n",
       "      <th>auc_405</th>\n",
       "      <th>auc_782</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mtlr</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.405</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.421</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model  bs_266  bs_405  bs_782  auc_266  auc_405  auc_782\n",
       "0  mtlr   0.051   0.092   0.139    0.405    0.305    0.421"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = []\n",
    "\n",
    "metrics.append({\n",
    "    \"model\": \"mtlr\",\n",
    "    **{f\"bs_{t}\": bs[i] for i, t in enumerate(eval_times)},\n",
    "    **{f\"auc_{t}\": auc[i] for i, t in enumerate(eval_times)}\n",
    "})\n",
    "\n",
    "pd.DataFrame(metrics).round(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import brier_score_loss, roc_auc_score\n",
    "\n",
    "def compute_metric_at_times(metric, time_true, prob_pred, event_observed, score_times):\n",
    "    \"\"\"Helper function to evaluate a metric at given timepoints.\"\"\"\n",
    "    scores = []\n",
    "    for time, pred in zip(score_times, prob_pred.T):\n",
    "        target = time_true > time\n",
    "        uncensored = target | event_observed.astype(bool)\n",
    "        scores.append(metric(target[uncensored], pred[uncensored]))\n",
    "        \n",
    "    return scores\n",
    "\n",
    "\n",
    "def brier_score_at_times(time_true, prob_pred, event_observed, score_times):\n",
    "    scores = compute_metric_at_times(brier_score_loss, \n",
    "                                     time_true,\n",
    "                                     prob_pred,\n",
    "                                     event_observed,\n",
    "                                     score_times)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def roc_auc_at_times(time_true, prob_pred, event_observed, score_times):\n",
    "    scores = compute_metric_at_times(roc_auc_score, \n",
    "                                     time_true,\n",
    "                                     prob_pred, \n",
    "                                     event_observed,\n",
    "                                     score_times)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHecking weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3212284/3960777235.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ct_weight = torch.load('/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT-CLIP_v2.pt')\n",
      "/tmp/ipykernel_3212284/3960777235.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  my_w = torch.load('/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/model_weights_new/weight_020.pth')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ct_weight = torch.load('/home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/CT-CLIP/CT-CLIP_v2.pt')\n",
    "my_w = torch.load('/opt/sagemaker/new_home/Mohammad.Qazi@mbzuai.ac.ae/project/ct_rate/save/model_weights_new/weight_020.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ct_weight.keys()\n",
    "\n",
    "# rename the keys of the model weights. add \"clip.base_model.model.\" to all the keys\n",
    "\n",
    "ct_weight_new = {}\n",
    "for k in ct_weight.keys():\n",
    "    ct_weight_new[\"clip.base_model.model.\"+k] = ct_weight[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['clip.base_model.model.temperature', 'clip.base_model.model.text_transformer.embeddings.position_ids', 'clip.base_model.model.text_transformer.embeddings.word_embeddings.weight', 'clip.base_model.model.text_transformer.embeddings.position_embeddings.weight', 'clip.base_model.model.text_transformer.embeddings.token_type_embeddings.weight', 'clip.base_model.model.text_transformer.embeddings.LayerNorm.weight', 'clip.base_model.model.text_transformer.embeddings.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.0.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.0.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.1.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.1.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.2.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.2.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.3.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.3.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.4.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.4.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.5.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.5.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.6.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.6.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.7.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.7.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.8.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.8.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.9.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.9.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.10.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.10.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.query.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.query.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.key.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.key.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.value.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.self.value.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.attention.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.intermediate.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.intermediate.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.output.dense.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.output.dense.bias', 'clip.base_model.model.text_transformer.encoder.layer.11.output.LayerNorm.weight', 'clip.base_model.model.text_transformer.encoder.layer.11.output.LayerNorm.bias', 'clip.base_model.model.text_transformer.pooler.dense.weight', 'clip.base_model.model.text_transformer.pooler.dense.bias', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.0.0.weight', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.0.0.bias', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.1.0.weight', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.1.0.bias', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.2.weight', 'clip.base_model.model.visual_transformer.spatial_rel_pos_bias.net.2.bias', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.1.weight', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.1.bias', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.2.weight', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.2.bias', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.3.weight', 'clip.base_model.model.visual_transformer.to_patch_emb_first_frame.3.bias', 'clip.base_model.model.visual_transformer.to_patch_emb.1.weight', 'clip.base_model.model.visual_transformer.to_patch_emb.1.bias', 'clip.base_model.model.visual_transformer.to_patch_emb.2.weight', 'clip.base_model.model.visual_transformer.to_patch_emb.2.bias', 'clip.base_model.model.visual_transformer.to_patch_emb.3.weight', 'clip.base_model.model.visual_transformer.to_patch_emb.3.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.null_kv', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.q_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.k_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.3.0.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.3.0.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.3.1.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.3.4.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.null_kv', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.q_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.k_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.3.0.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.3.0.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.3.1.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.3.4.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.null_kv', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.q_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.k_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.3.0.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.3.0.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.3.1.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.3.4.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.null_kv', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.q_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.k_scale', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.3.0.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.3.0.bias', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.3.1.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.3.4.weight', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.norm_out.gamma', 'clip.base_model.model.visual_transformer.enc_spatial_transformer.norm_out.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.null_kv', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.q_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.k_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.3.0.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.3.0.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.3.1.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.3.4.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.null_kv', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.q_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.k_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.3.0.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.3.0.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.3.1.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.3.4.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.null_kv', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.q_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.k_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.3.0.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.3.0.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.3.1.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.3.4.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.0.dsconv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.0.dsconv.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.null_kv', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.q_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.k_scale', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.context_norm.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.context_norm.beta', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.to_q.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.to_kv.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.to_out.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.3.0.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.3.0.bias', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.3.1.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.3.4.weight', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.norm_out.gamma', 'clip.base_model.model.visual_transformer.enc_temporal_transformer.norm_out.beta', 'clip.base_model.model.visual_transformer.vq._codebook.initted', 'clip.base_model.model.visual_transformer.vq._codebook.cluster_size', 'clip.base_model.model.visual_transformer.vq._codebook.embed', 'clip.base_model.model.visual_transformer.to_pixels_first_frame.0.weight', 'clip.base_model.model.visual_transformer.to_pixels_first_frame.0.bias', 'clip.base_model.model.visual_transformer.to_pixels.0.weight', 'clip.base_model.model.visual_transformer.to_pixels.0.bias', 'clip.base_model.model.to_text_latent.weight', 'clip.base_model.model.to_visual_latent.weight', 'clip.base_model.model.to_text_latent_extra.weight', 'clip.base_model.model.to_visual_latent_extra.weight'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ct_weight_new.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_q.base_layer.weight'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(my_w.keys())[228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_q.weight'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(ct_weight_new.keys())[228]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.0.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.1.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.2.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_spatial_transformer.layers.3.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.0.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.1.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.2.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.to_q.weight\n",
      "clip.base_model.model.visual_transformer.enc_temporal_transformer.layers.3.1.to_kv.weight\n",
      "clip.base_model.model.visual_transformer.vq._codebook.cluster_size 352\n",
      "clip.base_model.model.visual_transformer.vq._codebook.embed 353\n"
     ]
    }
   ],
   "source": [
    "for i, k in enumerate(ct_weight_new.keys()):\n",
    "    try:\n",
    "        if not torch.equal(ct_weight_new[k], my_w[k]):\n",
    "            print(k, i)\n",
    "    except:\n",
    "        print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "228"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
